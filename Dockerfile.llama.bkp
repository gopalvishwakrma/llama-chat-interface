# Dockerfile.llama
FROM almalinux:9

# Install system dependencies
RUN dnf update -y && \
    dnf groupinstall -y "Development Tools" && \
    dnf install -y \
        cmake \
        git \
        curl-devel \
        openssl-devel \
        wget \
        python3-pip && \
    dnf clean all

# Clone and build llama.cpp
WORKDIR /app
RUN git clone https://github.com/ggerganov/llama.cpp && \
    cd llama.cpp && \
    LLAMA_CURL=1 make

# Create models directory
RUN mkdir -p /models

# Download the model with retry and verification
RUN cd /models && \
    wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 -t 10 \
        https://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF/resolve/main/llama-3.2-1b-instruct-q8_0.gguf && \
    # Verify file exists and has size > 0
    test -s /models/llama-3.2-1b-instruct-q8_0.gguf

# Expose the server port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Start the server with reduced context size
WORKDIR /app/llama.cpp
CMD ["./llama-server", "-m", "/models/llama-3.2-1b-instruct-q8_0.gguf", "--host", "0.0.0.0", "--port", "8080", "-c", "512", "-t", "2"]
